\documentclass{article}

\title{Simultaneous Productions: \\ a Fully General Parsing Method to Make Progress on the Halting Problem}
\date{2020-06-08}
\author{Daniel McClanahan}

\begin{document}
\maketitle

\section{Relevant Prior Art / Notes}
\subsection{Papers}
\begin{itemize}
  \item \textbf{Freedom from the diagonal argument for circle-free Turing Machines:} \textit{../literature/Martin Davis - The Undecidable\_ Basic Papers on Undecidable Propositions, Unsolvable Problems and Computable Functions (2004)\_ocr.pdf} -- pages 137-138
  \item \textbf{Strong similarity to Petri Nets:} \textit{../literature/Memo-95-decision-problems-petri-nets\_ocr.pdf} -- pages 21-22
  \item \textbf{Potential freedom from principles II, III, IV of the Gandy automata criterion for computability:} \textit{../literature/gandy1980.pdf} -- specifically page 133, but really all four principles. further inline notes in notability app
  \item \textbf{Potential termination proof (or not) from plotkin's powerdomains:} \textit{../literature/plotkin-powerdomains-1976.pdf} -- page 4, specifically ``Now Konigâ€™s
lemma says that if every branch of a finitary tree is finite, then so is the tree
itself.''
\end{itemize}

\subsection{Wikipedia}
\begin{itemize}
  \item \textbf{descriptions of unbounded automata, especially regarding termination of ``infinite'' sequences:} \textit{https://en.wikipedia.org/wiki/Fair\_nondeterminism} -- notes on plotkin's result, as well as clinger: ``Though each node on an infinite branch must lie on a branch with a limit, the infinite branch need not itself have a limit. Thus the existence of an infinite branch does not necessarily imply a nonterminating computation.''
  \item \textbf{strong similarity to petri nets:} \textit{https://en.wikipedia.org/wiki/Petri\_nets}
  \item \textbf{the actor model doesn't really seem applicable, but the semantics of it might be:} \textit{https://en.wikipedia.org/wiki/Denotational\_semantics\_of\_the\_Actor\_model}
  \item \textbf{a closed actor system may represent S.P.:} \textit{https://en.wikipedia.org/wiki/Indeterminacy\_in\_concurrent\_computation}
  \item \textbf{chaitin's constant may be interesting:} \textit{https://en.wikipedia.org/wiki/Chaitin\%27s\_constant}
  \item \textbf{it seems godel's results may imply that computability is ``absolute'' -- is S.P. absolute?} \textit{https://en.wikipedia.org/wiki/Church\%E2\%80\%93Turing\_thesis\#complexity-theoretic\_Church\%E2\%80\%93Turing\_thesis}
  \item \textbf{recursively enumerable sets:} \textit{https://en.wikipedia.org/wiki/Recursively\_enumerable}
  \item \textbf{cantor's diagonal argument:} \textit{https://en.wikipedia.org/wiki/Cantor\%27s\_diagonal\_argument}
  \item \textbf{halting problem:} \textit{https://en.wikipedia.org/wiki/Halting\_problem} -- it's possible S.P. is immune to the naive result, as it can ``analyze'' the ``else: loop forever'' statement independently of the first branch
  \item \textbf{turing machine:} \textit{https://en.wikipedia.org/wiki/Turing\_machine}
  \item \textbf{quantified boolean formula:} \textit{https://en.wikipedia.org/wiki/Quantified\_Boolean\_formula\_problem} -- interesting thought experiment for universal quantifier -- is that the same power that S.P. has to infer a T.M.?
  \item \textbf{savitch's theorem:} \textit{https://en.wikipedia.org/wiki/Savitch\%27s\_theorem} (!!!) -- nondeterministic T.M.s only use a square root of the space of a deterministic T.M., as opposed to the possibly-exponential time bound difference between the two (!!!)
\end{itemize}

\section{Background}
This paper will prove that the \textit{S.P.} parsing algorithm is \textit{streamable} and \textit{cacheable}, and that the \textit{S.P.} grammar-grammar is recursively enumerable (i.e. that \textit{T.M.} can be reduced to \textit{S.P.}). We will then prove that \textit{streamability} and \textit{cacheability} are sufficient to produce a parsing algorithm that can handle stack cycles in linear (???/whatever runtime we find) time. Finally, we will demonstrate that a \textit{T.M.}'s runtime increases superlinearly (???) as $k$-context-sensitivity increases, thereby defining a strict superset of \textit{T.M.}s called \textit{S.P}s, of which \textit{S.P.} is a member.

\section{Concepts}
\subsection{The S.P. Grammar-Grammar}
\begin{itemize}
  \item \textit{Describe why it's called a grammar-grammar, then describe the elements of the (simple) grammar-grammar, including nonterminals, terminals, ellipses, cases, and productions.}
  \item \textit{Describe the relationship to the Chomsky formulation.}
  \item \textit{Describe what differs from the Chomsky formulation.}
  \item \textit{Describe the concept of stack cycles in an S.P. grammar.}
  \item \textit{Describe the grammar-grammar in relationship to an S.P. fully-realized ``grammar'' vs e.g. a context-free grammar.}
\end{itemize}

\subsection{\textit{Streamability} and \textit{Cacheability}}
\begin{itemize}
  \item \textit{Define streamability and cacheability as mathematical properties in terms of parsing algorithms in general.}
  \item \textit{The point of these is to parameterize the qualities that S.P. has which other parsing algorithms lack. The idea is to make it more clear that S.P. is a *paradigm* of parsing, not a single algorithm.}
  \item \textit{If possible, we want to prove the performance characteristics and correctness *in terms of* streamability and cacheability to demonstrate how to slot in a new ``backend'' for the algorithm.}
\end{itemize}

\subsection{$k$-context-sensitivity}
A context-sensitive language has at least one situation in which the parse tree can have multiple valid values for a sub-parse depending on the status of a super-parse. A $k$-context-sensitive language is one in which the depth of the stack that determines a sub-parse is bounded by a constant $k$. \textbf{TODO: VALIDATE!} In recursively enumerable languages, the depth of the stack of symbols needed to determine the correct sub-parse is instead bounded by the length of the input $n$.

\section{The S.P. Parsing Algorithm}
\subsection{Architecture Overview}
\textit{List and briefly describe the phases of the algorithm.}

\subsection{Data Structures and Techniques}
\begin{itemize}
  \item \textit{lexicographic BFS / partitioning (cite Spinrad's book, etc)}
\end{itemize}

\subsection{Phases}
\textit{This part should be useful for implementors of the algorithm.}
\subsubsection{Preprocessing the S.P. Grammar}
\subsubsection{Setting up a Parse}
\subsubsection{Parsing}
\subsubsection{Resolving the Matched Input}

\section{Correctness}
\subsection{Proof of Streamability and Cacheability}
\subsection{Equivalence of S.P and T.M.}
\begin{itemize}
  \item \textit{This demonstrates that S.P. can parse recursively enumerable languages.}
\end{itemize}
\subsection{Equivalence of Stack Cycles and $k$-context-sensitivity}

\section{Performance}
\subsection{Parsing a Context-Free Language}
\subsection{Parsing a $k$-Context-Sensitive Language}
\subsection{Parsing a Recursively Enumerable Language}
\textit{If ``$k$-context-sensitivity'' is general enough to cover this, we may not need a separate section.}

\subsection{Parallelism}
\textit{Describe the runtime of the algorithm if $k$ independent CPUs are provided, taking into account the cost of moving data across cores.}

\textit{This section will likely require an entirely separate analysis.} \textbf{NOTE: It might be extremely enlightening to make this the first goal in analyzing the algorithm, and then consider the single-CPU case as a special case.}

\section{Effect on the Halting Problem}
\begin{itemize}
  \item \textit{Describe/Prove how the Halting Problem applies to T.M.s vs S.P.s, referencing the Performance section.}
  \item \textit{Describe how S.P. was created by just thinking about having more than one state at a time (so giving insight into what underlying issues caused S.P. not to be found until now, and how others could have figured this out instead of me).}
  \item \textit{Technically, this makes T.M.s a strict subset (!!!) of S.P.s that can only have a single state at a time and can only respond to the halting problem by running forever.} \textbf{THIS IS SUPER IMPORTANT AND POWERFUL!!!} \textit{Make it clear that this applies to any construct that satisfies ``streamability'' and ``cacheability''.}
\end{itemize}

\section{Conclusions and Future Work}
\begin{itemize}
  \item \textit{Contrast S.P. to the Chomsky formulation.}
  \item \textit{We define the ``streamability'' and ``cacheability'' properties.}
  \item \textit{Those properties are shown to be (???) sufficient to create a construct better than a T.M.}
  \item \textit{S.P. is an example of this superior construct.}
  \item \textit{Describe how S.P. is the ``holy grail'' of parsing algorithms, and what parsing theory should focus on next.}
  \item \textit{Mention the benchmarks paper.}
  \item \textit{Mention running it backwards into a Monte Carlo Search Tree.}
\end{itemize}

\end{document}
