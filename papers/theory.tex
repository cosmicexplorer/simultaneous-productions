\documentclass{article}

\usepackage{hyperref}

\title{Simultaneous Productions: \\ a Fully General Parsing Method to Make Progress on the Halting Problem}
\date{2020-06-08}
\author{Daniel McClanahan}

\begin{document}
\maketitle

\section{Relevant Prior Art / Notes}
\subsection{Overview}

\textbf{Recall that since we tried comparing the S.P. parsing algorithm to a T.M., there are two completely different use cases we run into with prior art: \textit{parsing} and \textit{execution}.} Similarly, we find that references \textit{so far} can be divided into two categories: useful for \textit{completing the actual proof} and \textit{making the proof familiar to experts}.

\subsubsection{For Completing the Proof}
\label{completing-the-proof}

A \ref{petri-net} \textbf{Petri Net} is the closest model I have found to S.P.'s current \textit{evaluation} method, i.e. how it models the parsing algorithm \textit{after} preprocessing the provided S.P grammar, \textit{upon} a given input. However, a \ref{rcg} \textbf{Range Concatenation Grammar} appears to be the closest model I have found to the S.P. grammar-grammar (how grammars can be specified), and it seems close enough that \textbf{proofs of the RCG's Turing-equivalence may be transferable to S.P. (!!!!)}.

\subsubsection{For Making it Familiar}
\label{making-it-familiar}

The preprocessing phase of an S.P. grammar (TODO: write!) involves lots of graphs which may be unfamiliar to people more familiar with the Chomsky definition of a formal language. Defining it as a \ref{graph-rewriting} \textbf{Graph Rewriting System} (also called a ``graph grammar'') may help to explain the process that is being performed (TODO: write preprocessing section, describe it as \textit{performing a reachability analysis which also marks where any loops occur between ``states'' so the parsing process can know \textbf{in advance} before it goes into a loop mode!!!!!!!!!!!! this may be what makes it better than a T.M. aka fix the halting problem!!!!! this also may be what makes it ``less powerful'' than a T.M. \^\_\^}).

\subsection{Papers}
\begin{itemize}
  \item \textbf{Freedom from the diagonal argument for circle-free Turing Machines:} \textit{../literature/Martin Davis - The Undecidable\_ Basic Papers on Undecidable Propositions, Unsolvable Problems and Computable Functions (2004)\_ocr.pdf} -- pages 137-138
  \item \phantomsection \label{petri-net} \textbf{Strong similarity to Petri Nets:} \textit{../literature/Memo-95-decision-problems-petri-nets\_ocr.pdf} -- pages 21-22
    \textit{../literature/tr63\_ocr.pdf} -- ``the reachability problem for vector addition systems is shown to require at least exponential space'' -- unclear still how vector addition systems relate to S.P.
  \item \textbf{Potential freedom from principles II, III, IV of the Gandy automata criterion for computability:} \textit{../literature/gandy1980.pdf} -- specifically page 133, but really all four principles. further inline notes in notability app
  \item \textbf{Potential termination proof (or not) from plotkin's powerdomains:} \textit{../literature/plotkin-powerdomains-1976.pdf} -- page 4, specifically ``Now Konigâ€™s
lemma says that if every branch of a finitary tree is finite, then so is the tree
itself.''
  \item \textbf{Relationship of decidability to the 3-body problem:} \textit{../literature/dynsys.pdf} also see \url{https://twitter.com/hillelogram/status/1395847898739445761?s=20}
\end{itemize}

\subsection{Wikipedia}
\begin{itemize}
  \item \phantomsection \label{rcg} \textbf{description of a grammar-grammar VERY VERY SIMILAR to S.P.'s, which allows negation:} \url{https://en.wikipedia.org/wiki/Range_concatenation_grammars} -- \textit{without} negations is equivalent to a T.M., and a proof of equivalence could apply to S.P. (\textit{separate} from the question of termination)
  \item \phantomsection \label{graph-rewriting} \textbf{the concept of graph rewrite rules} \url{https://en.wikipedia.org/wiki/Graph\_grammar} -- intended to emphasize similarity to how grammars are defined as constructions/productions
  \item \textbf{descriptions of unbounded automata, especially regarding termination of ``infinite'' sequences:} \url{https://en.wikipedia.org/wiki/Fair_nondeterminism} -- notes on plotkin's result, as well as clinger: ``Though each node on an infinite branch must lie on a branch with a limit, the infinite branch need not itself have a limit. Thus the existence of an infinite branch does not necessarily imply a nonterminating computation.''
  \item \textbf{strong similarity to petri nets:} \url{https://en.wikipedia.org/wiki/Petri\_nets}
  \item \textbf{the actor model doesn't really seem applicable, but the semantics of it might be:} \url{https://en.wikipedia.org/wiki/Denotational\_semantics\_of\_the\_Actor\_model}
  \item \textbf{a closed actor system may represent S.P.:} \url{https://en.wikipedia.org/wiki/Indeterminacy\_in\_concurrent\_computation}
  \item \textbf{chaitin's constant may be interesting:} \url{https://en.wikipedia.org/wiki/Chaitin\%27s\_constant}
  \item \textbf{it seems godel's results may imply that computability is ``absolute'' -- is S.P. absolute?} \url{https://en.wikipedia.org/wiki/Church\%E2\%80\%93Turing\_thesis\#complexity-theoretic\_Church\%E2\%80\%93Turing\_thesis}
  \item \textbf{recursively enumerable sets:} \url{https://en.wikipedia.org/wiki/Recursively\_enumerable}
  \item \textbf{cantor's diagonal argument:} \url{https://en.wikipedia.org/wiki/Cantor\%27s\_diagonal\_argument}
  \item \textbf{halting problem:} \url{https://en.wikipedia.org/wiki/Halting\_problem} -- it's possible S.P. is immune to the naive result, as it can ``analyze'' the ``else: loop forever'' statement independently of the first branch
  \item \textbf{turing machine:} \url{https://en.wikipedia.org/wiki/Turing\_machine}
  \item \textbf{quantified boolean formula:} \url{https://en.wikipedia.org/wiki/Quantified\_Boolean\_formula\_problem} -- interesting thought experiment for universal quantifier -- is that the same power that S.P. has to infer a T.M.?
  \item \textbf{savitch's theorem:} \url{https://en.wikipedia.org/wiki/Savitch\%27s\_theorem} (!!!) -- nondeterministic T.M.s only use a square root of the space of a deterministic T.M., as opposed to the possibly-exponential time bound difference between the two (!!!)
\end{itemize}

\subsection{Twitter}
\begin{itemize}
  \item \textbf{several string literal optimization mechanisms (aka SIMD stuff)} \url{https://twitter.com/geofflangdale/status/1399894038698860545} -- \textit{internal links are: \url{https://eprints.whiterose.ac.uk/109809/1/jsre_journal_accepted_author_manuscript.pdf} and \url{https://nitely.github.io/2020/11/30/regex-literals-optimization.html}}
\end{itemize}

\section{Background}
This paper will prove that the \textit{S.P.} parsing algorithm is \textit{streamable} and \textit{cacheable}, and that the \textit{S.P.} grammar-grammar is recursively enumerable (i.e. that \textit{T.M.} can be reduced to \textit{S.P.}). We will then prove that \textit{streamability} and \textit{cacheability} are sufficient to produce a parsing algorithm that can handle stack cycles in linear (???/whatever runtime we find) time. Finally, we will demonstrate that a \textit{T.M.}'s runtime increases superlinearly (???) as $k$-context-sensitivity increases, thereby defining a strict superset of \textit{T.M.}s called \textit{S.P}s, of which \textit{S.P.} is a member.

\section{Concepts}
\subsection{The S.P. Grammar-Grammar}
\begin{itemize}
  \item \textit{Describe why it's called a grammar-grammar, then describe the elements of the (simple) grammar-grammar, including nonterminals, terminals, ellipses, cases, and productions.}
  \item \textit{Describe the relationship to the Chomsky formulation.}
  \item \textit{Describe what differs from the Chomsky formulation.}
  \item \textit{Describe the concept of stack cycles in an S.P. grammar.}
  \item \textit{Describe the grammar-grammar in relationship to an S.P. fully-realized ``grammar'' vs e.g. a context-free grammar.}
\end{itemize}

\subsection{\textit{Streamability} and \textit{Cacheability}}
\begin{itemize}
  \item \textit{Define streamability and cacheability as mathematical properties in terms of parsing algorithms in general.}
  \item \textit{The point of these is to parameterize the qualities that S.P. has which other parsing algorithms lack. The idea is to make it more clear that S.P. is a *paradigm* of parsing, not a single algorithm.}
  \item \textit{If possible, we want to prove the performance characteristics and correctness *in terms of* streamability and cacheability to demonstrate how to slot in a new ``backend'' for the algorithm.}
\end{itemize}

\subsection{$k$-context-sensitivity}
A context-sensitive language has at least one situation in which the parse tree can have multiple valid values for a sub-parse depending on the status of a super-parse. A $k$-context-sensitive language is one in which the depth of the stack that determines a sub-parse is bounded by a constant $k$. \textbf{TODO: VALIDATE!} In recursively enumerable languages, the depth of the stack of symbols needed to determine the correct sub-parse is instead bounded by the length of the input $n$.

\section{The S.P. Parsing Algorithm}
\subsection{Architecture Overview}
\textit{List and briefly describe the phases of the algorithm.}

\subsection{Data Structures and Techniques}
\begin{itemize}
  \item \textit{lexicographic BFS / partitioning (cite Spinrad's book, etc)}
\end{itemize}

\subsection{Phases}
\textit{This part should be useful for implementors of the algorithm.}
\subsubsection{Preprocessing the S.P. Grammar}
\subsubsection{Setting up a Parse}
\subsubsection{Parsing}
\subsubsection{Resolving the Matched Input}

\section{Correctness}
\subsection{Proof of Streamability and Cacheability}
\subsection{Equivalence of S.P and T.M.}
\begin{itemize}
  \item \textit{This demonstrates that S.P. can parse recursively enumerable languages.}
\end{itemize}
\subsection{Equivalence of Stack Cycles and $k$-context-sensitivity}

\section{Performance}
\subsection{Parsing a Context-Free Language}
\subsection{Parsing a $k$-Context-Sensitive Language}
\subsection{Parsing a Recursively Enumerable Language}
\textit{If ``$k$-context-sensitivity'' is general enough to cover this, we may not need a separate section.}

\subsection{Parallelism}
\textit{Describe the runtime of the algorithm if $k$ independent CPUs are provided, taking into account the cost of moving data across cores.}

\textit{This section will likely require an entirely separate analysis.} \textbf{NOTE: It might be extremely enlightening to make this the first goal in analyzing the algorithm, and then consider the single-CPU case as a special case.}

\section{Effect on the Halting Problem}
\begin{itemize}
  \item \textit{Describe/Prove how the Halting Problem applies to T.M.s vs S.P.s, referencing the Performance section.}
  \item \textit{Describe how S.P. was created by just thinking about having more than one state at a time (so giving insight into what underlying issues caused S.P. not to be found until now, and how others could have figured this out instead of me).}
  \item \textit{Technically, this makes T.M.s a strict subset (!!!) of S.P.s that can only have a single state at a time and can only respond to the halting problem by running forever.} \textbf{THIS IS SUPER IMPORTANT AND POWERFUL!!!} \textit{Make it clear that this applies to any construct that satisfies ``streamability'' and ``cacheability''.}
\end{itemize}

\section{Conclusions and Future Work}
\begin{itemize}
  \item \textit{Contrast S.P. to the Chomsky formulation.}
  \item \textit{We define the ``streamability'' and ``cacheability'' properties.}
  \item \textit{Those properties are shown to be (???) sufficient to create a construct better than a T.M.}
  \item \textit{S.P. is an example of this superior construct.}
  \item \textit{Describe how S.P. is the ``holy grail'' of parsing algorithms, and what parsing theory should focus on next.}
  \item \textit{Mention the benchmarks paper.}
  \item \textit{Mention running it backwards into a Monte Carlo Search Tree.}
\end{itemize}

\end{document}
