\documentclass[10pt]{article}

\usepackage{mcclanahan}

\title{Simultaneous Productions: \\ A Fully General Grammar Specification}
\date{2021-06-10}
\author{Danny McClanahan}

\newcommand{\todocite}[1]{\footnote{cite: #1}}

\newcommand{\leftmost}[1]{\text{leftmost}(#1)}
\newcommand{\rightmost}[1]{\text{rightmost}(#1)}
\newcommand{\generalsubseq}{\overbar{I}}
\newcommand{\leftsubseq}{\generalsubseq}
\newcommand{\rightsubseq}{\generalsubseq'}

\newcommand{\leftleft}{\leftmost{\leftsubseq}}
\newcommand{\leftrignt}{\leftmost{\rightsubseq}}
\newcommand{\rightleft}{\rightmost{\leftsubseq}}
\newcommand{\rightright}{\rightmost{\rightsubseq}}

\newcommand{\lookingleft}{\text{Left}}
\newcommand{\lookingright}{\text{Right}}
\newcommand{\lookingat}{L_{\updownarrow}}
\newcommand{\lookdirection}{\{\lookingleft, \lookingright\}}
\newcommand{\lookdirectionset}{\{ \text{LookDirection} \}}

\newcommand{\subseqset}{\{\generalsubseq\}}
\newcommand{\subseqsetpair}{\subseqset \times \subseqset}

\newcommand{\opsuccess}{\text{Success}}
\newcommand{\opfailure}{\text{Failure}}
\newcommand{\ophasnoclue}{\text{IDK}}
\newcommand{\maybeopspace}{\{\opsuccess, \opfailure, \ophasnoclue\}}
\newcommand{\opresultset}{\{ \text{Result} \}}

\newcommand{\substringwith}[2]{\overbar{I}_{(#1,#2)}}
\newcommand{\canonicalleftend}{l_1}
\newcommand{\canonicalrightend}{l_2}
\newcommand{\canonicalsubstring}{\substringwith{\canonicalleftend}{\canonicalrightend}}
\newcommand{\bookmarkwith}[1]{\widehat{I}_{(#1)}}
\newcommand{\canonicalbookmarkindex}{l^+}
\newcommand{\canonicalbookmark}{\bookmarkwith{\canonicalbookmarkindex}}

\begin{document}
\maketitle
\tableofcontents

\section{Motivation for a New Grammar Specification}
\label{sec:motivation}

This paper is the first of several on a parsing method we will refer to as ``Simultaneous Productions'' (or ``S.P.'' for short). This name was chosen to emphasize two goals of this method:
\begin{enumerate}
  \item An S.P. \textit{grammar} is composed of a set of \textit{productions}, very similar to most existing concepts of formal grammars \todocite{chomsky formal grammars}. However, unlike many common parsing algorithms \todocite{common parsing algorithms -- frequency and power? maybe cite history of parsing page?}, an S.P. grammar can represent a recursively enumerable language \todocite{what is RecEnum?}.
  \item When parsing a string, these productions can be independently evaluated over separate parts of the input string, and adjacent successful matches can then be merged to form a successful parse. Unlike many common parsing algorithms \todocite{might need to expand on this more? need to think about what ``common'' means}, this feature avoids any intrinsic serial dependencies that require parsing the beginning of the string first, hence allowing for \textit{parallelism} during the parsing process.
\end{enumerate}

\subsection{Goals}
\label{sec:goals}
We have noted that the above two features are not shared by many commonly-used parsing algorithms \todocite{the history of parsing webpage}. In this paper, we will describe the S.P. \textit{grammar-grammar}, i.e. the specification which defines any S.P. grammar. We hope to show that:
\begin{enumerate}
  \item The S.P. grammar-grammar should be proven equivalent to Chomsky's canonical specification of an formal language \todocite{chomsky formal grammars again, maybe just turing machine?}.
  \item The S.P. grammar input format should be able to represent all rungs of the Chomsky hierarchy distinctly to the \textit{grammar-writer}, who may then pick and choose the kind of complexity they want instead of getting surprised. For example: \begin{itemize}
      \item regular expressions (DFAs) \todocite{what are DFAs/regex}, \\
      \item EBNF syntax commonly used for CFGs \todocite{use of EBNF for CFGs}, \\
      \item regex with backrefs (recursively enumerable?) \todocite{backrefs are RecEnum?}.
    \end{itemize}
  \item Furthermore, representing a grammar with S.P. \textbf{should follow strongly a ``pay for what you use'' policy}, i.e. having more complex features should not create needless complexity for users who only use the simpler features. \textbf{This requires creating a hierarchical-ish concept of feature complexity.}
\end{enumerate}

\subsection{No Parsing Allowed}
\label{sec:no-parsing-allowed}
\textbf{We will not rely on the details of any specific parsing algorithm.} Indeed, we will avoid introducing any iterative algorithms at all in this paper. Further paper(s) will describe an efficient parsing algorithm to evaluate an S.P. grammar over a specific input string.

The intention of this separation is to allow the S.P. grammar-grammar to be reviewed and criticized separately from the evaluation method. This is done because the author believes that the S.P. grammar-grammar has merit in itself, as a ``lingua franca'' for \textit{executable formal grammars}, that is, grammars which can be efficiently parsed by computer.

\subsection{Background: Composable Grammars}
\label{sec:background-composable-grammars}
The S.P. model was partially created to make grammar definitions significantly more composable by \textbf{separating the entry point for parsing/string matching from the rest of the grammar's definition}. The hope is that this can allow for \textbf{global agreement upon a safe reliable subset of ``primitive'' parsers (e.g. integer and float parsers) across codebases}, while end users are still able to add custom logic on top seamlessly.

Similarly, we describe our goal to separate the output of a parse from the algorithm that produces it in \explicitsecref{sec:no-parsing-allowed}. We hope that \textbf{standardizing parser output} into the fully general \footnote{is this true?} PoP format as well as \textbf{sharing high-quality grammars in a modular way} can reduce what we see as a frustrating amount of mental overhead and duplicated work for everyone who wants to write any sort of compiler or interpreter. We believe this situation can be improved.

\subsection{Notation}
\label{sec:notation}
\begin{itemize}
  \item Named concepts in the rest of this paper will be represented in \textit{italics} when first defined.
  \item Capital letters generally refer to sets, while lowercase letters generally refer to elements of some set. This may not be true in all cases.
  \item As abbreviations: \begin{itemize}
    \item $[n] = [1, n] \forall n \in \N$.
    \item $\pipe$ should be translated as ``for some'' or ``for which''.
  \end{itemize}
\end{itemize}

\section{Definition}
\label{sec:definition}
We first define the \textit{S.P. grammar-grammar}, as a kind of meta-grammar which specifies all concrete S.P. grammars. We use the term grammar-grammar to emphasize the regular structure of an S.P. grammar. We believe this formulation is relatively simple to analyze, and in subsequent work we will demonstrate that it admits a relatively performant \textit{parsing algorithm}, or \textit{evaluation method}. It is possible this representation can be further improved.

\subsection{S.P. Grammar-Grammar}
\label{sec:grammar-grammar}
\begin{equation}
  \label{eq:sp}
  SP = (\Sigma, \scr{P}).
\end{equation}
An \textit{S.P. grammar} $SP$ is a 2-tuple with an arbitrary finite set $\Sigma$ and a finite set of productions $\scr{P}$ defined in \expliciteqnref{eq:p}. We refer to $\Sigma$ as the \textit{alphabet}.

\begin{equation}
  \label{eq:p}
  p = \scr{C}_p \forall p \in \scr{P}.
\end{equation}
Each \textit{production} $p$ is a finite set of cases $\scr{C}_p$, defined in \expliciteqnref{eq:cp}.

\begin{equation}
  \label{eq:cp}
  c_p = \{e_j\}_{j=1}^m \forall c_p \in \scr{C}_p.
\end{equation}
Each \textit{case} $c_p$ is a finite sequence of case elements $\{e_j\}_{j=1}^m$, where $m$ is the number of elements in the sequence $c_p$, with $e_j$ defined in \expliciteqnref{eq:ej}.

\begin{equation}
  \label{eq:ej}
  e_j = \begin{alignedcases}{c}
    t \in \Sigma, \\
    p \in \scr{P}.
  \end{alignedcases} \forall j \in [m].
\end{equation}
Each \textit{case element} $e_j$ is either a \textit{terminal} $t \in \Sigma$ or \textit{nonterminal} $p \in \scr{P}$.

\section{PoP: Proof of Parsing}
\label{sec:proof-of-parsing}
We describe a tree-like data structure ``proof of parsing'' (PoP) which retains sufficient information to validate that a string belongs to a recursively enumerable language, as well as the validation function ``validate'' applied to the data structure.

% $p$ \textit{matches} an input string $I$
% Each production $p \in \scr{P}$ can be \textit{matched} against some input string $I$ when \textbf{any} case $c_p \in \scr{C}_p$ matches $I$. $c_p$ matches $I$ iff \textbf{all} terminals and nonterminals are matched against consecutive non-overlapping subsequences of the input string $I$.

\subsection{Grammar Specialization}
\label{sec:grammar-specialization}
\begin{equation}
  \label{eq:p-top}
  p^* \in \scr{P}.
\end{equation}
An S.P. grammar $SP$ alone is not sufficient information to unambiguously parse a string -- a specific production must also be provided (we have defined it to be this way). Therefore to get an \textit{executable grammar}, we select a single ``top'' production $p^* \in \scr{P}$. This is vaguely reminiscent of the \textit{start symbol} found in Chomsky grammars \explicitsecref{sec:chomsky-equivalence}.

\textbf{TODO: this mechanism is the same way we should use for specifying precedence of non-top productions when a parse is completely ambiguous in a subtree!!!}

\begin{equation}
  \label{eq:specialized}
  SP^* = (\Sigma, \scr{P}, p^*).
\end{equation}
The tuple $SP^*$ formed from the selection of $p^*$ is referred to as a \textit{specialized grammar}. For this reason, we may also refer to a grammar $SP$ (without having chosen any $p^*$ yet) as an \textit{unspecialized grammar}.

\subsection{Input Specification}
\label{sec:input-specification}
\begin{equation}
  \label{eq:input-string-tokens}
  I = \{t_i\}_{i=1}^n \pipe t_i \in \Sigma \forall i \in [n].
\end{equation}
An \textit{input string} $I$ is a finite sequence of \textit{tokens} $\{t_i\}_{i=1}^n$ from the alphabet $\Sigma$, where $\abs{I} = n$ is the number of elements in the sequence $I$.

\textbf{TODO: we should not distinguish the act of parsing a finite vs infinite stream!}

% \subsection{Partitioning the Input}
% \label{sec:partitioning-the-input}
% We would like to be able to reason independently about how different subsequences $\generalsubseq$ of the input $I$ may match a certain production $p \in \scr{P}$. We eventually make use of this reasoning in \explicitsecref{sec:matching-a-string}, where we describe how to check whether an arbitrary production $p$ matches an arbitrary $\generalsubseq$. In later work we hope to show that this enables highly scalable performance gains through caching and parallelism techniques.

% \subsubsection{Substrings, Bookmarks, and Subsequences}
% \label{sec:subsequences}
% We define two methods to represent a \textit{subsequence} $\generalsubseq$ of the input string $I$: \textit{substrings} $\canonicalsubstring$ and \textit{bookmarks} $\canonicalbookmark$.

% \begin{align}
%   \label{eq:subsequences}
%   \generalsubseq_{\canonicalleftend,l_2} &= \{t_i\}_{i=\canonicalleftend}^{l_2} &&= \text{substring}(I, \canonicalleftend, l_2) &&&\pipe &&&\canonicalleftend \le l_2 \le n \in \N. \\
%   \canonicalbookmark &= \{\} &&= \text{bookmark}(I, \canonicalbookmarkindex) &&&\pipe &&&\canonicalbookmarkindex \in [n + 1]. \\
%   \subseqset &= \{\generalsubseq_{\canonicalleftend,l_2}\} \disjcup \{\canonicalbookmark\} &&= \text{subsequences}(I).
% \end{align}
% The \textit{substring} $\generalsubseq_{\canonicalleftend,l_2}$ is the subsequence of $I$ from indices $\canonicalleftend$ to $l_2$, inclusive. The \textit{bookmark} $\canonicalbookmark$ is an empty sequence (technically an empty subsequence of $I$) which is inserted \textbf{before} the index $\canonicalbookmarkindex$. In the case that $\canonicalbookmarkindex = n + 1$, the bookmark $\canonicalbookmark$ is considered to be at the \textbf{end} of the input string $I$.

% We use the notation $\subseqset$ to denote the disjoint union of these two types of \textit{subsequences} of $I$. Bookmarks are essentially only needed to represent productions which match the empty string (which are perfectly legal): see the matching process in \explicitsecref{sec:matching-a-string}.

% \subsubsection{Sorting Subsequences}
% \label{sec:sorting-subsequences}
% We would like to be able to compare subsequences from separate, possibly-overlapping parts of the string, in order to produce a data structure that looks like a ``parse tree'', but which can also represent non-local dependencies, such as those found in context-sensitive and recursively enumerable languages \todocite{cite or prove for context-sensitivity requiring non-local deps!}.

% We first establish the ``leftmost'' and ``rightmost'' functions, and introduce the concept of ``adjacency'' for subsequences. We then produce an ``adjacency mapping'' construct which splits up the input $I$  \explicitsecref{sec:adjacency-mapping}.

% \begin{align}
%   \label{eq:leftmost-rightmost-span}
%   \begin{array}{rcl}
%     \begin{alignedcases}{lclcl}
%       \leftmost{\canonicalbookmark} &=& \canonicalbookmarkindex &\in& [n + 1], \\
%       \leftmost{\canonicalsubstring} &=& \canonicalleftend &\in& [n].
%     \end{alignedcases} &=& \leftmost{\generalsubseq} \forall \generalsubseq \in \subseqset. \\
%     \leftmost{\generalsubseq} &:& \subseqset \rightarrow [n + 1].
%   \end{array} \\
%   \begin{array}{rcl}
%     \begin{alignedcases}{lclcl}
%       \rightmost{\canonicalbookmark} &=& \canonicalbookmarkindex &\in& [n + 1], \\
%       \rightmost{\canonicalsubstring} &=& \canonicalrightend &\in& [n].
%     \end{alignedcases} &=& \rightmost{\generalsubseq} \forall \generalsubseq \in \subseqset. \\
%     \rightmost{\generalsubseq} &:& \subseqset \rightarrow [n + 1].
%   \end{array}
% \end{align}
% \textbf{TODO: ???}

% \begin{align}
%   \label{eq:f-minus}
%   \lookdirectionset &= \lookdirection. \\
%   \opresultset &= \maybeopspace. \\
%   F_-^+ &: \p{\subseqsetpair \times \lookdirection} \rightarrow \maybeopspace. \\
%   F_-(\leftsubseq, \rightsubseq) &= F_-^+(\leftsubseq, \rightsubseq, \lookingleft). \\
%   F^+(\leftsubseq, \rightsubseq) &= F_-^+(\leftsubseq, \rightsubseq, \lookingright). \\
%   F_-(\leftsubseq, \rightsubseq) &= \begin{alignedcases}{lcr}
%     \leftleft = \leftrignt &\Rightarrow& \opfailure, \\
%     \leftleft < \leftrignt &\Rightarrow& \opsuccess, \\
%     F_-(\rightsubseq, \leftsubseq) = \opsuccess &\Rightarrow& \opfailure, \\
%     \otherwise &\Rightarrow& \ophasnoclue.
%   \end{alignedcases} \\
%   F^+(\leftsubseq, \rightsubseq) &= \begin{alignedcases}{lcr}
%     \rightleft = \rightright &\Rightarrow& \opfailure, \\
%     \rightleft < \rightright &\Rightarrow& \opsuccess, \\
%     F^+(\rightsubseq, \leftsubseq) = \opsuccess &\Rightarrow& \opfailure, \\
%     \otherwise &\Rightarrow& \ophasnoclue.
%   \end{alignedcases}
% \end{align}
% The functions $F_-$ and $F^+$ provide ``less than'' and ``greater than'' operators which can compare any two subsequences of $I$, but may return an $\ophasnoclue$ result.

% \subsection{Adjacency}
% \label{sec:adjacency}
% We now attempt to strengthen our concept of ordering to represent \textit{adjacent} subsequences $\generalsubseq$ of $I$. We use this later in our definition of an adjacency mapping $\generalsubseq^*_k$ which ``spans'' the entire input $I$ \explicitsecref{sec:adjacency-mapping}.

% \subsubsection{Adjacent Subsequences}
% \label{sec:adjacent-subsequences}
% \begin{equation}
%   \label{eq:adjacent-subsequences}
%   \begin{array}{rl}
%     \subseqset \times \subseqset \rightarrow \ternarylogicspace &: \text{adjacent}. \\
%     \begin{alignedcases}{lcrcr}
%       \generalsubseq = \canonicalbookmark, \generalsubseq' = \widehat{I}_{{\canonicalbookmarkindex}'} &\Rightarrow& \canonicalbookmarkindex = {\canonicalbookmarkindex}' &\Leftrightarrow& \true, \\
%       \generalsubseq = \canonicalbookmark, \generalsubseq' = \generalsubseq_{\canonicalleftend',l_2'} &\Rightarrow& \canonicalbookmarkindex = \canonicalleftend' &\Leftrightarrow& \true, \\
%       \generalsubseq = \generalsubseq_{\canonicalleftend,l_2}, \generalsubseq' = \widehat{I}_{{\canonicalbookmarkindex}'} &\Rightarrow& l_2 = {\canonicalbookmarkindex}' + 1 &\Leftrightarrow& \true, \\
%       \generalsubseq = \generalsubseq_{\canonicalleftend,l_2}, \generalsubseq' = \generalsubseq_{\canonicalleftend',l_2'} &\Rightarrow& \canonicalleftend' = l_2 + 1 &\Leftrightarrow& \true, \\
%       \otherwise &\Rightarrow& && \undefval.
%     \end{alignedcases} &= \text{adjacent}(\generalsubseq, \generalsubseq').
%   \end{array}
% \end{equation}
% Two subsequences $\generalsubseq$ and $\generalsubseq'$ of $I$ are \textit{adjacent} when $\text{adjacent}(\generalsubseq, \overbar{I'}) = \true$. A bookmark is adjacent to another bookmark when they occupy the same position within $I$. A bookmark is adjacent to a substring when it is immediately before or immediately after the substring. Two substrings are adjacent when the end of one substring is immediately before the beginning of the other.

% \subsubsection{Adjacency Mapping}
% \label{sec:adjacency-mapping}
% \begin{align}
%   \label{eq:adjacency-mapping}
%   \{\generalsubseq_q\}_{q=1}^k &= \generalsubseq^*_k. \\
%   \begin{alignedcases}{rl}
%     \text{left boundary condition: } \generalsubseq_1 &= \begin{alignedcases}{c}
%       \widehat{I}_{(\canonicalbookmarkindex = 1)}, \text{ or} \\
%       \generalsubseq_{(\canonicalleftend=1),(l_2 \in [n])}.
%     \end{alignedcases},\text{ and} \\
%     \text{right boundary condition: } \generalsubseq_k &= \begin{alignedcases}{c}
%       \widehat{I}_{(\canonicalbookmarkindex = n+1)}, \text{ or} \\
%       \generalsubseq_{(\canonicalleftend \in [n]),(l_2=n)}.
%     \end{alignedcases},\text{ and} \\
%     \text{contiguous: } \text{adjacent}(\generalsubseq_q, \generalsubseq_{q + 1}) &= \true \forall q \in [k - 1].
%   \end{alignedcases} &\Leftrightarrow \generalsubseq^*_k \text{ spans } I.
% \end{align}
% An \textit{adjacency mapping} $\generalsubseq^*_k$ is a \textit{contiguous} or \textit{consecutively adjacent} sequence of length $k$ of subsequences $\{\generalsubseq_q\}_{q=1}^k$ of the input string $I$, in which the first element $\generalsubseq_1$ is adjacent to, or contains, the first element $t_1$ of $I$, and the final element $\generalsubseq_k$ is adjacent to, or contains, the last element $t_n$ of $I$. A bookmark for $\generalsubseq_1$ or $\generalsubseq_k$ would be adjacent to $t_1$ or $t_n$, while a substring would contain $t_1$ or $t_n$. We say that $\generalsubseq^*_k$ \textit{spans} the tokens of $I$.

\subsection{Matching a String}
\label{sec:matching-a-string}
We represent the conditions necessary to match a production $p \in \scr{P}$ recursively, by defining the ``matches'' function over multiple separate domains:
\begin{enumerate}
  \item $\text{matches}_{(\scr{P})}$: the top-level function that matches against a single production $p$.
  \item $\text{matches}_{(\scr{C}_p)}$: matches against a single case $c_p$ from a production $p$.
  \item $\text{matches}_{(e_j)}$: matches against a single case element $e_j$ from the case $c_p$.
\end{enumerate}

\subsubsection{Matching a Production $p \in \scr{P}$}
\label{sec:matching-a-production-p}
\begin{equation}
  \label{eq:matches-prod}
  \begin{array}{rl}
    \text{matches}_{(\scr{P})} &: \scr{P} \times \{I\} \rightarrow \binaryspace. \\
    \text{matches}_{(\scr{P})}(p, I) &= \{ \exists c_p \in \scr{C}_p \pipe \text{matches}_{(\scr{C}_p)}(c_p, I) \Leftrightarrow \true \}.
  \end{array}
\end{equation}
A production $p \in \scr{P}$ \textit{matches} an input string $I$ when \textbf{any} of its cases $c_p \in \scr{C}_p$ match $I$ via $\text{matches}_{(\scr{C}_p)}$ as defined in \expliciteqnref{eq:matches-case}.

\subsubsection{Matching a Case $c_p \in \scr{C}_p$}
\label{sec:matching-a-case-cp-in-scrcp}
\begin{equation}
  \label{eq:matches-case}
  \begin{array}{rl}
    \text{matches}_{(\scr{C}_p)} &: \scr{C}_p \times \{I\} \rightarrow \binaryspace. \\
    \text{matches}_{(\scr{C}_p)}(c_p, I) &= \{ \text{\textbf{TODO: ???}} \}.
    % \text{matches}_{(\scr{C}_p)}(c_p, I) &= \{ \exists \generalsubseq^*_m \pipe \text{matches}_{(e_j)}(e_j, \generalsubseq_j) \forall j \in [m] \Leftrightarrow \true \}.
  \end{array}
\end{equation}
% A case $c_p \in \scr{C}_p$ \textit{matches} an input string $I$ when there exists an adjacency mapping $\generalsubseq^*_m$ of length $m = \abs{c_p}$ which maps each case element $e_j$ to a subsequence $\generalsubseq_j$ such that every case element matches its assigned subsequence from the adjacency mapping as defined in \expliciteqnref{eq:matches-element}.

\textbf{TODO: >= context-sensitive can be modelled as coroutines, compared to the immediate function call of DFA/CFGs?}

\subsubsection{Matching a Case Element $e_j \in c_p$}
\label{sec:matching-a-case-element-ej-in-cp}
\begin{equation}
  \label{eq:matches-element}
  \begin{array}{rl}
    [m] \times \subseqset \rightarrow \binaryspace &: \text{matches}_{(e_j)}. \\
    \begin{alignedcases}{lcrcl}
      e_j = t \in \Sigma &\Rightarrow& \generalsubseq = \generalsubseq_{\canonicalleftend,l_2}, \canonicalleftend = l_2, I_{\canonicalleftend} = t &\Leftrightarrow& \true, \\
      e_j = p' \in \scr{P} &\Rightarrow& \text{matches}_{(\scr{P})}(SP, p', \generalsubseq) &\Leftrightarrow& \true.
    \end{alignedcases} &= \text{matches}_{(e_j)}(j, \generalsubseq).
  \end{array}
\end{equation}
A case element $e_j$ matches an input subsequence $\generalsubseq$ when $e_j$ is a token $t \in \Sigma$, in which case $\generalsubseq$ is a length-1 substring of $I$ containing the single token $t$, or when $e_j$ is a production $p'$, in which case the subsequence $\generalsubseq$ must match the production $p'$ as defined in \expliciteqnref{eq:matches-prod}.

\subsection{Summary of Adjacency}
\label{sec:summary-of-adjacency}
At this stage, we note a few important points:
\begin{enumerate}
  % \item An adjacency mapping $\generalsubseq^*_k$ essentially represents a parse tree \textbf{TODO: HOW??? CROSS-SERIAL DEPS???}
  \item A production $p$ may match a finite or countably infinite number of subsequences $\generalsubseq$ of $I$, not just one. So, if we say $p$ matches $\generalsubseq$ for some case $c_p$, it may still match other subsequences $\generalsubseq'$, either for the same case $c_p$, or other cases $c'_p \in \scr{C}_p$.
  % \item We have not yet described a method to \textbf{actually construct an adjacency mapping $\generalsubseq^*$ for a given specialized grammar and input}. That is out of scope for this paper.
\end{enumerate}

\section{Proof of Turing-Equivalence}
\label{sec:proof-of-turing-equivalence}

\textbf{TODO: do reduction from \href{https://en.wikipedia.org/wiki/Lambda_calculus}{lambda calculus!!}}

\section{Chomsky Equivalence}
\label{sec:chomsky-equivalence}
We have described an S.P. grammar $SP = (\Sigma, \scr{P})$ \explicitsecref{sec:grammar-grammar}, and we have \textit{specialized} the grammar into $SP^* = (\Sigma, \scr{P}, p^*)$ by selecting a production $p \in \scr{P}$ \explicitsecref{sec:grammar-specialization}. We have described the conditions under which $p^*$ is said to successfully match an input string $I$ consisting of tokens from $\Sigma$ \explicitsecref{sec:matching-a-string}.

We attempt to directly reduce the canonical specification of a formal grammar (often attributed to Noam Chomsky) into specialized or executable form $SP^*$ \todocite{chomsky grammars}.

\subsection{Chomsky Construct}
\label{sec:chomsky-construct}
The definition of a ``formal grammar'' we copy from \textbf{TODO: cite!!!} as follows \todocite{chomsky!!!}:

\subsubsection{Formal Grammar Definition}
\label{sec:formal-grammar-definition}
\begin{align}
  \label{eq:chomsky-production}
  \Sigma &\text{: terminal symbols.} \\
  N &\text{: nonterminal symbols.} \\
  S &\text{: start symbol.} \in N \\
  P &\text{: productions.} \\
  P &= \{\p{\Sigma\, \cup \, N}^* N \p{\Sigma\, \cup \, N}^* \rightarrow \p{\Sigma\, \cup \, N}^* \}. \\
  G &= (N, \Sigma, P, S).
\end{align}

\subsubsection{Parsing a Formal Grammar}
\label{sec:proof-of-parsing-a-formal-grammar}
asdf

\subsection{Equivalence Proof}
\label{sec:equivalence-proof}
We will perform a Cook-Levin-style reduction from a Chomsky grammar $G = (N, \Sigma, P, S)$ \explicitsecref{sec:chomsky-construct} into a specialized S.P grammar $SP^* = (\Sigma, \scr{P}, p^*)$ \explicitsecref{sec:grammar-specialization} \todocite{cook-levin!}.

\subsubsection{Construction of $\Sigma_{SP^*}$}
\label{sec:construction-of-sigma}
\begin{equation}\label{eq:alphabet}
  \Sigma_{SP^*} = \Sigma_G.
\end{equation}
The alphabet $\Sigma$ is exactly the same in both S.P. and the Chomsky formulation.

\subsubsection{Construction of $\scr{P}$}
\label{sec:construction-of-p}
\begin{equation}
  \label{eq:productions-reduction}
  asdf
\end{equation}

\section{Relevant Prior Art / Notes}
\subsection{Overview}
\label{sec:overview}

asdf

\end{document}
